#!/usr/bin/env python
# coding: utf-8

# In[1]:


# å¯¼å…¥éœ€è¦ç”¨åˆ°çš„åº“
get_ipython().system('pip install seaborn')
get_ipython().system('pip install xgboost')
get_ipython().system('pip install statsmodels')
get_ipython().system('pip install lightgbm')
import pandas as pd
import scutquant2.scutquant as q
from scutquant2 import alpha, report, executor


# # ğŸ”¥ hsy æ•°æ®ç»“æ„

# ## è¯»å–è‚¡ä»·å†å²é‡ä»·æ•°æ®
# 
# 

# In[2]:


import pandas as pd

read_path = "autodl-tmp/Dataset/ä¸ªè‚¡é‡ä»·æ•°æ®/2014-2023.csv"

# è¯»å–CSVæ–‡ä»¶ï¼Œå¹¶æŒ‡å®šSymbolåˆ—çš„æ•°æ®ç±»å‹ä¸ºå­—ç¬¦ä¸²
df = pd.read_csv(read_path, dtype={'Symbol': str})
df['Symbol'] = df['Symbol'].astype(str)  # ç¡®ä¿Symbolåˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
print(df['æ—¥æœŸ'].min())  # æ‰“å°df1ä¸­æ—¥æœŸçš„æœ€å°å€¼


# æ‰“å°å‰20è¡Œæ•°æ®
print(df.head(3))


# In[3]:


df = df.rename(columns={'Symbol':'instrument','æ—¥æœŸ': 'datetime', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close', 'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume', 'æˆäº¤é¢': 'amount', 'æŒ¯å¹…': 'amplitude', 'æ¶¨è·Œå¹…': 'pct_chg', 'æ¶¨è·Œé¢': 'change', 'æ¢æ‰‹ç‡': 'turnover'})

df.head(10)
df = df[df["datetime"] < "2023-12-31"]
df


# In[4]:


df.set_index(["datetime", "instrument"], inplace=True)
# df["turnover_20std"] = df['turnover'].groupby(level='instrument').rolling(window=20).std().droplevel(0)
# df['turnover_20mean'] = df['turnover'].groupby(level='instrument').rolling(window=20).mean().droplevel(0)
# df['turnover_std/mean'] =df["turnover_20std"]/df["turnover_20mean"]


# In[5]:


df.dropna(inplace=True)
df.head(3)


# In[6]:


# æ„é€ ç›®æ ‡å€¼, å¹¶æŒ‰ç…§-10% å’Œ10%æˆªæ–­(å› ä¸ºæœ‰æ¶¨è·Œåœé™åˆ¶)
df["label"] = df["pct_chg"].groupby("instrument").shift(-2) / 100
df = df.dropna()
df["label"].clip(-0.1, 0.1, inplace=True) 
df["label"].describe()


# In[7]:


kwargs = {
    "data": df,
    "open": "open",
    "close": "close",
    "high": "high",
    "low": "low",
    "volume": "volume",
    "amount": "amount",
    "amplitude": "amplitude",
    "turnover": "turnover",
    "groupby": "instrument"
    
}

X = alpha.qlib158(df)
X = pd.concat([X, df[["label"]]], axis=1)
X = X.dropna()
X.reset_index(inplace=True)
X


# ## è¯»å–æ–°é—»æ ‡é¢˜æƒ…æ„ŸæŒ‡æ•°

# ### æ•´åˆæ–‡ä»¶

# In[8]:


import pandas as pd
import os


base_dir = "autodl-tmp/Dataset/aggregated_sentiment"  # åŸºç¡€ç›®å½•
file_pattern = "{}_aggregated.csv"  # æ–‡ä»¶åæ¨¡å¼ï¼ˆå¹´ä»½å ä½ç¬¦ï¼‰
years = range(2014, 2024)  # 2014åˆ°2023
file_paths = [os.path.join(base_dir, file_pattern.format(year)) for year in years]

# 3. è¯»å–æ‰€æœ‰CSVæ–‡ä»¶å¹¶åˆå¹¶
dfs = []  # å­˜å‚¨æ‰€æœ‰DataFrameçš„åˆ—è¡¨
for file_path in file_paths:
    try:
        df = pd.read_csv(file_path, dtype={'Symbol': str}) 
        df = df.rename(columns={'Symbol': 'instrument'})
        df['instrument'] = df['instrument'].astype(str)
        dfs.append(df)
    except FileNotFoundError:
        print(f"è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡: {file_path}")
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

# 4. åˆå¹¶æ‰€æœ‰DataFrameå¹¶å‘½åä¸ºdf_news
if dfs:
    df_news = pd.concat(dfs, ignore_index=True)
    # ç¡®ä¿'instrument'åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ˆåŒé‡ä¿é™©ï¼‰
    df_news['instrument'] = df_news['instrument'].astype(str)
else:
    raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯åˆå¹¶çš„æ–‡ä»¶ï¼")

# 5. åˆ—åè°ƒæ•´
df_news.drop(columns=["total_word_count"], inplace=True)
df_news = df_news[["instrument","DeclareDate", "sentiment"]].rename(columns={
    "DeclareDate": "datetime",
    "sentiment": "sentiment_index"
})
df_news.head(10)

# 6. æ‰“å°åˆå¹¶åçš„ç»“æœ
print("åˆå¹¶åçš„df_newså½¢çŠ¶:", df_news.shape)
print("\nå‰5è¡Œæ•°æ®:")
df_news


# ### æ–°é—»æ•°æ®å’Œè‚¡ç¥¨é‡ä»·æ•°æ®å¯¹é½
# 

# In[9]:


import pandas as pd

# å‡è®¾df_newsæ˜¯ä¸€ä¸ªpandas DataFrameï¼ŒåŒ…å«'instrument', 'datetime'å’Œ'sentiment_index'åˆ—
# é¦–å…ˆï¼Œç¡®ä¿'datetime'åˆ—çš„ç±»å‹ä¸ºdatetime
df_news['datetime'] = pd.to_datetime(df_news['datetime'])

# åˆ›å»ºä¸€ä¸ªæ–°çš„DataFrameï¼ŒåŒ…å«æ‰€æœ‰'instrument'å’Œä»'2014-01-01'åˆ°'2023-12-31'çš„æ—¥æœŸ
unique_instruments = df_news['instrument'].unique()
all_dates = pd.date_range(start='2014-01-01', end='2023-12-31')

# ä½¿ç”¨ç¬›å¡å°”ç§¯åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„'instrument'å’Œ'date'ç»„åˆ
full_df = pd.MultiIndex.from_product([unique_instruments, all_dates], names=['instrument', 'datetime']).to_frame(index=False)

# ç¡®ä¿full_dfçš„'datetime'åˆ—æ˜¯datetimeç±»å‹
full_df['datetime'] = pd.to_datetime(full_df['datetime'])

# åˆå¹¶df_newsåˆ°full_dfï¼Œä¿ç•™æ‰€æœ‰full_dfçš„è¡Œï¼Œå¹¶å°†ç¼ºå¤±çš„'sentiment_index'å€¼å¡«å……ä¸º0
df_news = pd.merge(full_df, df_news, on=['instrument', 'datetime'], how='left').fillna({'sentiment_index': 0})

# æ‰“å°æ›´æ–°åçš„DataFrameçš„å‰20è¡Œ
df_news.head(3)



# In[10]:


X = X.reset_index() 
# ç¡®ä¿Symbolåˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
df_news['instrument'] = df_news['instrument'].astype(str)
X['instrument'] = X['instrument'].astype(str)

# ç¡®ä¿datetimeåˆ—æ˜¯datetimeç±»å‹
df_news['datetime'] = pd.to_datetime(df_news['datetime'])
X['datetime'] = pd.to_datetime(X['datetime'])

#ç­›é€‰dfå’Œdf_newsä¸¤ä¸ªdataframeä¸­instrumentç›¸åŒä¸”datetimeç›¸åŒçš„è¡Œï¼Œç„¶åæŠŠXä¸­çš„æ•°æ®åˆå¹¶åˆ°df_newsä¸­
merged_df = pd.merge(df_news, X, on=['instrument', 'datetime'], how='inner')
merged_df


# In[11]:


print(max(merged_df["sentiment_index"]))


# In[12]:


del X
del df_news


# In[13]:


merged_df['instrument'] = merged_df['instrument'].astype(str)
merged_df.set_index(["datetime", "instrument"], inplace=True)
print(merged_df.head(10))


# # ğŸ”¥hjl æœºå™¨å­¦ä¹ é¢„æµ‹å› å­

# ## æ¨¡å‹é¢„æµ‹

# In[14]:


merged_df.dropna(inplace=True)


# In[15]:


import pandas as pd

split_params = {
    "test_start_date": "2022-01-01",  # æµ‹è¯•é›†çš„å¼€å§‹æ—¥æœŸï¼Œæ­¤æ—¥æœŸä¹‹åçš„æ•°æ®å½’å…¥æµ‹è¯•é›†
    "split_method": "split_by_date", 
    "split_kwargs": {
        "train_end_date": "2021-01-01",  # è®­ç»ƒé›†çš„ç»“æŸæ—¥æœŸï¼Œæ­¤æ—¥æœŸä¹‹å‰çš„æ•°æ®ç”¨äºè®­ç»ƒ
        "valid_end_date": "2022-01-01"   # éªŒè¯é›†çš„ç»“æŸæ—¥æœŸï¼Œä»‹äºtrain_end_dateå’Œæ­¤æ—¥æœŸä¹‹é—´çš„æ•°æ®ç”¨äºéªŒè¯
    }
}


# æŒ‰split_paramsæ‹†åˆ†å‡ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†, å¹¶åœ¨æˆªé¢ä¸Šè¿›è¡Œæ ‡å‡†åŒ–
result = q.auto_process(merged_df, "label", groupby="instrument", split_params=split_params, select=False, orth=False, clip=5)

X_train, y_train, X_valid, y_valid = result["X_train"], result["y_train"], result["X_valid"], result["y_valid"]
X_test, y_test, ymean, ystd = result["X_test"], result["y_test"], result["ymean"], result["ystd"]


# In[16]:


# ç»§ç»­åç»­çš„å¤„ç†
X_train, y_train, X_valid, y_valid
X_test, y_test, ymean, ystd


# In[17]:


# é€‰æ‹©ä¸€ä¸ªå› å­, å¹¶å¯¹å…¶å¯è§†åŒ– (æ­¤æ­¥éª¤éå¿…è¦)
report.single_factor_ana(X_train["sentiment_index"])


# In[18]:


# æ‹Ÿåˆæ¨¡å‹
model = q.auto_lgbm(X_train, y_train, X_valid, y_valid, early_stopping=50)

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
pred = model.predict(X_test)
pred = pd.DataFrame(pred, columns=["predict"], index=X_test.index)

# ä¸ºé˜²æ­¢æ•°æ®æ³„éœ²ï¼Œä½¿ç”¨æ»å2é˜¶çš„meanå’Œstdè¿˜åŸé¢„æµ‹å€¼
pred.rename_axis(['datetime', 'instrument'], inplace=True)

pred["predict"] += ymean.groupby("datetime").shift(2).fillna(0.0002)
pred["predict"] *= ystd.groupby("datetime").shift(2).fillna(0.0189)

# è¾“å‡ºé¢„æµ‹ç»“æœçš„æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯
pred.describe()


# In[19]:


pred.head(10)


# # ğŸ”¥wxx æŠ•èµ„åº”ç”¨

# In[20]:


# ä½¿ç”¨ICè¯„åˆ¤æ‹Ÿåˆä¼˜åº¦
ic, icir, rank_ic, rank_icir = q.ic_ana(pred, y_test, groupby='datetime')
print('ic=', ic, 'icir=', icir, 'rank_ic=', rank_ic, 'rank_icir=', rank_icir)
# ä½¿ç”¨pearsonç›¸å…³ç³»æ•°è¯„åˆ¤æ‹Ÿåˆä¼˜åº¦
r = q.pearson_corr(pred["predict"], y_test)
r


# In[21]:


merged_df.head(10)


# In[22]:


pred.head()


# In[23]:


df_test = merged_df[merged_df.index.isin(pred.index)]
df_test.head(10)


# In[24]:


# ä»¥ä¸‹ä¸ºå›æµ‹çš„æ•°æ®æ ¼å¼è½¬æ¢éƒ¨åˆ†
# å°†é¢„æµ‹å€¼å¤„ç†æˆå›æµ‹éœ€è¦çš„æ ¼å¼(éœ€è¦çŸ¥é“é¢„æµ‹å€¼"predict", äº¤æ˜“ä»·æ ¼priceå’Œäº¤æ˜“é‡volume)è¿™è¾¹çš„dataåŒ…å«datetimeå’Œclose
# df_test = merged_df[merged_df.index.isin(pred.index)]
"""
    :param predict: pd.DataFrame, é¢„æµ‹å€¼, åº”åŒ…æ‹¬"predict"
    :param data: pd.DataFrame, æä¾›æ—¶é—´å’Œä»·æ ¼ä¿¡æ¯
    :param price: str, dataä¸­è¡¨ç¤ºä»·æ ¼çš„åˆ—å
    :param volume: str, dataä¸­è¡¨ç¤ºæˆäº¤é‡çš„åˆ—å
    :param real_ret: pd.Series, çœŸå®æ”¶ç›Šç‡
    :return: pd.DataFrame
    """
data_ = df_test.copy()
data_.head()


# In[25]:


pred.columns = ["predict"]
index = pred.index
data1 = data_[data_.index.isin(index)]
data1 = data1.reset_index()
data1 = data1.set_index(pred.index.names).sort_index()
data1.head()


# In[26]:


pred.head()


# In[27]:


# å°† 'y_test' è½¬æ¢ä¸º DataFrame å¹¶è®¾ç½®åˆ—åç§°ä¸º 'R'
y_test = y_test.to_frame(name='R')
y_test.head()


# In[28]:


data1.head()


# In[29]:


# ç¡®ä¿ 'pred' çš„ç´¢å¼•æ˜¯å”¯ä¸€çš„
# ä½¿ç”¨ merge æ–¹æ³•åˆå¹¶ 'pred' å’Œ 'data1'ï¼ŒåŸºäºå®ƒä»¬å…±æœ‰çš„ç´¢å¼•
pred.index.names = ["datetime", "instrument"]
pred = pred.merge(data1[['close', 'volume']], left_index=True, right_index=True, how='left')


pred.rename(columns={'close': 'price'}, inplace=True)
pred.index.names = ["time", "code"]
y_test.index.names = ["time", "code"]
pred["price"] = pred['price'].groupby(["code"]).shift(-1) # æŒ‡ä»¤æ˜¯Tæ—¶ç”Ÿæˆçš„, ä½†æ˜¯T+1æ‰§è¡Œ, æ‰€ä»¥æ˜¯shift(-1)
pred = pred.merge(y_test[['R']], left_index=True, right_index=True, how='left')  # æœ¬æ¥å°±æ˜¯T+2å¯¹T+1çš„æ”¶ç›Šç‡, å› æ­¤ä¸ç”¨å‰ç§»
pred.head(10)


# In[ ]:


pred = pred[~pred.index.duplicated(keep='first')]
pred.sort_index()


# In[ ]:


y_test = y_test[~y_test.index.duplicated(keep='first')]
y_test.sort_index()
y_test.dropna(inplace=True)
y_test_series = y_test.squeeze()
y_test_series


# In[ ]:


pred.dropna(inplace=True)
pred


# In[ ]:


# ä»¥ä¸‹ä¸ºå›æµ‹çš„ç­–ç•¥æ‰§è¡Œéƒ¨åˆ†
# å°†é¢„æµ‹å€¼å¤„ç†æˆå›æµ‹éœ€è¦çš„æ ¼å¼(éœ€è¦çŸ¥é“é¢„æµ‹å€¼"predict", äº¤æ˜“ä»·æ ¼priceå’Œäº¤æ˜“é‡volume)è¿™è¾¹çš„dataåŒ…å«datetimeå’Œclose
# df_test = merged_df[merged_df.index.isin(pred.index)]
# pred = executor.prepare(pred, df_test, price='close', volume='volume', real_ret=y_test)

pred.dropna(inplace=True)
# å¦‚æœå‡ºç°æ˜æ˜¾çš„åˆ†å±‚, åˆ™è¯´æ˜å› å­æœ‰è‰¯å¥½çš„é€‰è‚¡èƒ½åŠ›
report.group_return_ana(pred, y_test_series)

backtest = {
    "generator": {
        "mode": "generate"
    },
    "strategy": {
        "class": "TopKStrategy",  # åšå¤šé¢„æµ‹å€¼å‰20%è‚¡ç¥¨, åšç©ºå20%çš„è‚¡ç¥¨. è‡ªåŠ¨å¹³ä»“
        "kwargs": {
            "k": 0.2,
            "auto_offset": False,
            "offset_freq": 2, # åº”ä¸ºdelta_t + 1, ä¾‹å¦‚ç›®æ ‡å€¼æ˜¯close_-2 / close_-1 - 1, åˆ™delta_t = 1
            "buy_only": False,  # =Trueæ—¶ï¼Œåªåšå¤šä¸åšç©º(åœ¨Aè‚¡åšç©ºæœ‰ä¸€å®šçš„éš¾åº¦)
            "short_volume": 500, # èåˆ¸åšç©ºçš„æ•°é‡
            "risk_degree": 0.95,  # å°†é£é™©åº¦æ§åˆ¶åœ¨è¿™ä¸ªæ•°ï¼Œå¦‚æœè¶…è¿‡äº†å°±æŒ‰æ¯”ä¾‹å‡æŒè‚¡ç¥¨ç›´åˆ°é£é™©åº¦å°äºç­‰äºå®ƒä¸ºæ­¢
            "unit": None,  # ç”±äºæ•°æ®å·²ç»æ˜¯ä»¥æ‰‹ä¸ºå•ä½, æ•…æ— éœ€äºŒæ¬¡å¤„ç†
            "max_volume": 0.05  # æ‰‹æ•°éšå¯ç”¨èµ„é‡‘è€Œæ”¹å˜ï¼Œæœ€å¤§ä¸ä¼šè¶…è¿‡è‚¡ç¥¨å½“å¤©æˆäº¤é‡çš„1%(ä¾‹å¦‚T+1æ—¶ä¸‹å•ï¼Œä¸‹å•æ‰‹æ•°ä¸ä¼šè¶…è¿‡Tæ—¶æˆäº¤é‡çš„1%)
        }
    },
    "account": None, # ä½¿ç”¨é»˜è®¤è´¦æˆ·, å³åˆå§‹èµ„é‡‘ä¸º1äº¿, æ— åº•ä»“ (æ³¨æ„ç­–ç•¥å®¹é‡ï¼)
    "trade_params": {
        "cost_buy": 0.0015,  # ä½£é‡‘åŠ ä¸Šå°èŠ±ç¨
        "cost_sell": 0.0015,
        "min_cost": 5,
    }
}
generator, strategy, account, trade_params = backtest["generator"], backtest["strategy"], backtest["account"], backtest["trade_params"]
exe = executor.Executor(generator, strategy, account, trade_params)
exe.execute(data=pred, verbose=0)  # verbose=1æ—¶ï¼ŒæŒ‰æ—¶é—´è¾“å‡ºä¹°å–æŒ‡ä»¤

# æŠ¥å‘Šå›æµ‹ç»“æœ
user_account, benchmark = exe.user_account, exe.benchmark
report.report_all(user_account, benchmark, freq=1, time=exe.time,rf=0.01 )  # æ— é£é™©æ”¶ç›Šç‡é»˜è®¤ä¸ºå¹´åŒ–3%ï¼Œå¯ä»¥é€šè¿‡å‚æ•°rfè°ƒèŠ‚


# In[ ]:




